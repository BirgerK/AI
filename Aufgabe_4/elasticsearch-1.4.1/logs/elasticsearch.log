[2014-12-11 18:52:57,787][INFO ][node                     ] [Roma] version[1.4.1], pid[6008], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-11 18:52:57,787][INFO ][node                     ] [Roma] initializing ...
[2014-12-11 18:52:57,790][INFO ][plugins                  ] [Roma] loaded [], sites []
[2014-12-11 18:53:01,383][INFO ][node                     ] [Roma] initialized
[2014-12-11 18:53:01,384][INFO ][node                     ] [Roma] starting ...
[2014-12-11 18:53:01,524][INFO ][transport                ] [Roma] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.12:9300]}
[2014-12-11 18:53:01,648][INFO ][discovery                ] [Roma] elasticsearch/h2fmpZkmThORzAHAHUr8Sg
[2014-12-11 18:53:05,413][INFO ][cluster.service          ] [Roma] new_master [Roma][h2fmpZkmThORzAHAHUr8Sg][BlueHorst][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-11 18:53:05,437][INFO ][gateway                  ] [Roma] recovered [0] indices into cluster_state
[2014-12-11 18:53:05,490][INFO ][http                     ] [Roma] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.12:9200]}
[2014-12-11 18:53:05,491][INFO ][node                     ] [Roma] started
[2014-12-11 18:58:00,394][INFO ][node                     ] [Roma] stopping ...
[2014-12-11 18:58:00,406][INFO ][node                     ] [Roma] stopped
[2014-12-11 18:58:00,406][INFO ][node                     ] [Roma] closing ...
[2014-12-11 18:58:00,410][INFO ][node                     ] [Roma] closed
[2014-12-11 18:58:17,862][INFO ][node                     ] [Decay] version[1.4.1], pid[3024], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-11 18:58:17,863][INFO ][node                     ] [Decay] initializing ...
[2014-12-11 18:58:17,866][INFO ][plugins                  ] [Decay] loaded [], sites []
[2014-12-11 18:58:20,697][INFO ][node                     ] [Decay] initialized
[2014-12-11 18:58:20,698][INFO ][node                     ] [Decay] starting ...
[2014-12-11 18:58:20,822][INFO ][transport                ] [Decay] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.12:9300]}
[2014-12-11 18:58:20,940][INFO ][discovery                ] [Decay] elasticsearch/HqgQ3XK2SBmmEUKnqvorrA
[2014-12-11 18:58:24,704][INFO ][cluster.service          ] [Decay] new_master [Decay][HqgQ3XK2SBmmEUKnqvorrA][BlueHorst][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-11 18:58:24,724][INFO ][gateway                  ] [Decay] recovered [0] indices into cluster_state
[2014-12-11 18:58:24,778][INFO ][http                     ] [Decay] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.12:9200]}
[2014-12-11 18:58:24,778][INFO ][node                     ] [Decay] started
[2014-12-11 19:00:06,086][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x64b01adc, /192.168.1.12:2330 => /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:06,090][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x64b01adc, /192.168.1.12:2330 :> /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:06,090][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x75baa748, /192.168.1.12:2331 => /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:06,099][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x75baa748, /192.168.1.12:2331 :> /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:06,100][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x1f7280df, /192.168.1.12:2332 => /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:06,108][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x1f7280df, /192.168.1.12:2332 :> /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:06,140][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x80006128, /192.168.1.12:2333 => /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:06,141][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x80006128, /192.168.1.12:2333 :> /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:11,144][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x17c57861, /192.168.1.12:2334 => /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:11,146][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0x17c57861, /192.168.1.12:2334 :> /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:32,533][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0xb488742c, /192.168.1.12:2335 => /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:32,537][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0xb488742c, /192.168.1.12:2335 :> /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:32,538][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0xbb7afed1, /192.168.1.12:2336 => /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:32,556][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0xbb7afed1, /192.168.1.12:2336 :> /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:32,557][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0xdc700299, /192.168.1.12:2337 => /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:32,569][WARN ][transport.netty          ] [Decay] exception caught on transport layer [[id: 0xdc700299, /192.168.1.12:2337 :> /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:00:34,937][INFO ][node                     ] [Decay] stopping ...
[2014-12-11 19:00:34,946][INFO ][node                     ] [Decay] stopped
[2014-12-11 19:00:34,947][INFO ][node                     ] [Decay] closing ...
[2014-12-11 19:00:34,950][INFO ][node                     ] [Decay] closed
[2014-12-11 19:00:59,630][INFO ][node                     ] [Star-Lord] version[1.4.1], pid[5888], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-11 19:00:59,631][INFO ][node                     ] [Star-Lord] initializing ...
[2014-12-11 19:00:59,634][INFO ][plugins                  ] [Star-Lord] loaded [], sites []
[2014-12-11 19:01:02,485][INFO ][node                     ] [Star-Lord] initialized
[2014-12-11 19:01:02,485][INFO ][node                     ] [Star-Lord] starting ...
[2014-12-11 19:01:02,618][INFO ][transport                ] [Star-Lord] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.12:9300]}
[2014-12-11 19:01:02,742][INFO ][discovery                ] [Star-Lord] elasticsearch/aCZrkZYXSD-kqap9_Olpaw
[2014-12-11 19:01:06,506][INFO ][cluster.service          ] [Star-Lord] new_master [Star-Lord][aCZrkZYXSD-kqap9_Olpaw][BlueHorst][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-11 19:01:06,528][INFO ][gateway                  ] [Star-Lord] recovered [0] indices into cluster_state
[2014-12-11 19:01:06,581][INFO ][http                     ] [Star-Lord] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.12:9200]}
[2014-12-11 19:01:06,581][INFO ][node                     ] [Star-Lord] started
[2014-12-11 19:02:07,243][WARN ][transport.netty          ] [Star-Lord] exception caught on transport layer [[id: 0xf9654923, /192.168.1.12:2374 :> /192.168.1.12:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 19:05:47,231][INFO ][node                     ] [Star-Lord] stopping ...
[2014-12-11 19:05:47,240][INFO ][node                     ] [Star-Lord] stopped
[2014-12-11 19:05:47,240][INFO ][node                     ] [Star-Lord] closing ...
[2014-12-11 19:05:47,244][INFO ][node                     ] [Star-Lord] closed
[2014-12-11 19:08:59,939][INFO ][node                     ] [Capricorn] version[1.4.1], pid[5708], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-11 19:08:59,940][INFO ][node                     ] [Capricorn] initializing ...
[2014-12-11 19:08:59,942][INFO ][plugins                  ] [Capricorn] loaded [], sites []
[2014-12-11 19:09:02,792][INFO ][node                     ] [Capricorn] initialized
[2014-12-11 19:09:02,792][INFO ][node                     ] [Capricorn] starting ...
[2014-12-11 19:09:02,925][INFO ][transport                ] [Capricorn] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.12:9300]}
[2014-12-11 19:09:03,052][INFO ][discovery                ] [Capricorn] elasticsearch/Mi3-lZvpS-au4QyrF03Nkg
[2014-12-11 19:09:06,815][INFO ][cluster.service          ] [Capricorn] new_master [Capricorn][Mi3-lZvpS-au4QyrF03Nkg][BlueHorst][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-11 19:09:06,836][INFO ][gateway                  ] [Capricorn] recovered [0] indices into cluster_state
[2014-12-11 19:09:06,891][INFO ][http                     ] [Capricorn] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.12:9200]}
[2014-12-11 19:09:06,891][INFO ][node                     ] [Capricorn] started
[2014-12-11 19:17:48,466][INFO ][node                     ] [Capricorn] stopping ...
[2014-12-11 19:17:48,477][INFO ][node                     ] [Capricorn] stopped
[2014-12-11 19:17:48,477][INFO ][node                     ] [Capricorn] closing ...
[2014-12-11 19:17:48,482][INFO ][node                     ] [Capricorn] closed
[2014-12-11 19:24:33,612][INFO ][node                     ] [Robert "Bobby" Drake] version[1.4.1], pid[2600], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-11 19:24:33,613][INFO ][node                     ] [Robert "Bobby" Drake] initializing ...
[2014-12-11 19:24:33,616][INFO ][plugins                  ] [Robert "Bobby" Drake] loaded [], sites []
[2014-12-11 19:24:36,450][INFO ][node                     ] [Robert "Bobby" Drake] initialized
[2014-12-11 19:24:36,451][INFO ][node                     ] [Robert "Bobby" Drake] starting ...
[2014-12-11 19:24:36,578][INFO ][transport                ] [Robert "Bobby" Drake] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.12:9300]}
[2014-12-11 19:24:36,698][INFO ][discovery                ] [Robert "Bobby" Drake] elasticsearch/fox1zwEERQ2IfQZNqvS7Xg
[2014-12-11 19:24:40,462][INFO ][cluster.service          ] [Robert "Bobby" Drake] new_master [Robert "Bobby" Drake][fox1zwEERQ2IfQZNqvS7Xg][BlueHorst][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-11 19:24:40,484][INFO ][gateway                  ] [Robert "Bobby" Drake] recovered [0] indices into cluster_state
[2014-12-11 19:24:40,537][INFO ][http                     ] [Robert "Bobby" Drake] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.12:9200]}
[2014-12-11 19:24:40,537][INFO ][node                     ] [Robert "Bobby" Drake] started
[2014-12-11 19:35:02,866][INFO ][node                     ] [Robert "Bobby" Drake] stopping ...
[2014-12-11 19:35:02,875][INFO ][node                     ] [Robert "Bobby" Drake] stopped
[2014-12-11 19:35:02,876][INFO ][node                     ] [Robert "Bobby" Drake] closing ...
[2014-12-11 19:35:02,879][INFO ][node                     ] [Robert "Bobby" Drake] closed
[2014-12-11 19:42:59,696][INFO ][node                     ] [Alibar] version[1.4.1], pid[6352], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-11 19:42:59,697][INFO ][node                     ] [Alibar] initializing ...
[2014-12-11 19:42:59,700][INFO ][plugins                  ] [Alibar] loaded [], sites []
[2014-12-11 19:43:02,527][INFO ][node                     ] [Alibar] initialized
[2014-12-11 19:43:02,527][INFO ][node                     ] [Alibar] starting ...
[2014-12-11 19:43:02,652][INFO ][transport                ] [Alibar] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.12:9300]}
[2014-12-11 19:43:02,772][INFO ][discovery                ] [Alibar] elasticsearch/MS6gaxyEQxiDDTHKQhs2BQ
[2014-12-11 19:43:06,535][INFO ][cluster.service          ] [Alibar] new_master [Alibar][MS6gaxyEQxiDDTHKQhs2BQ][BlueHorst][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-11 19:43:06,558][INFO ][gateway                  ] [Alibar] recovered [0] indices into cluster_state
[2014-12-11 19:43:06,610][INFO ][http                     ] [Alibar] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.12:9200]}
[2014-12-11 19:43:06,611][INFO ][node                     ] [Alibar] started
[2014-12-11 19:43:31,798][INFO ][node                     ] [Alibar] stopping ...
[2014-12-11 19:43:31,808][INFO ][node                     ] [Alibar] stopped
[2014-12-11 19:43:31,808][INFO ][node                     ] [Alibar] closing ...
[2014-12-11 19:43:31,813][INFO ][node                     ] [Alibar] closed
[2014-12-11 19:43:46,638][INFO ][node                     ] [Dark Phoenix] version[1.4.1], pid[4332], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-11 19:43:46,638][INFO ][node                     ] [Dark Phoenix] initializing ...
[2014-12-11 19:43:46,642][INFO ][plugins                  ] [Dark Phoenix] loaded [], sites []
[2014-12-11 19:43:49,477][INFO ][node                     ] [Dark Phoenix] initialized
[2014-12-11 19:43:49,477][INFO ][node                     ] [Dark Phoenix] starting ...
[2014-12-11 19:43:49,604][INFO ][transport                ] [Dark Phoenix] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.12:9300]}
[2014-12-11 19:43:49,724][INFO ][discovery                ] [Dark Phoenix] elasticsearch/cTLwxm-1TBKIvlmoKBTIPg
[2014-12-11 19:43:53,488][INFO ][cluster.service          ] [Dark Phoenix] new_master [Dark Phoenix][cTLwxm-1TBKIvlmoKBTIPg][BlueHorst][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-11 19:43:53,511][INFO ][gateway                  ] [Dark Phoenix] recovered [0] indices into cluster_state
[2014-12-11 19:43:53,563][INFO ][http                     ] [Dark Phoenix] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.12:9200]}
[2014-12-11 19:43:53,564][INFO ][node                     ] [Dark Phoenix] started
[2014-12-11 19:46:05,068][INFO ][node                     ] [Master Menace] version[1.4.1], pid[5852], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-11 19:46:05,068][INFO ][node                     ] [Master Menace] initializing ...
[2014-12-11 19:46:05,071][INFO ][plugins                  ] [Master Menace] loaded [], sites []
[2014-12-11 19:46:07,898][INFO ][node                     ] [Master Menace] initialized
[2014-12-11 19:46:07,898][INFO ][node                     ] [Master Menace] starting ...
[2014-12-11 19:46:08,026][INFO ][transport                ] [Master Menace] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.12:9300]}
[2014-12-11 19:46:08,147][INFO ][discovery                ] [Master Menace] elasticsearch/KP99cKXYRLaddutN2lpY2Q
[2014-12-11 19:46:11,910][INFO ][cluster.service          ] [Master Menace] new_master [Master Menace][KP99cKXYRLaddutN2lpY2Q][BlueHorst][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-11 19:46:11,933][INFO ][gateway                  ] [Master Menace] recovered [0] indices into cluster_state
[2014-12-11 19:46:11,985][INFO ][http                     ] [Master Menace] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.12:9200]}
[2014-12-11 19:46:11,986][INFO ][node                     ] [Master Menace] started
[2014-12-11 20:19:56,480][INFO ][cluster.metadata         ] [Master Menace] [shakespere] creating index, cause [api], shards [5]/[1], mappings []
[2014-12-11 20:24:43,367][DEBUG][action.admin.indices.create] [Master Menace] [shakespeare -d ' {  "mappings" : {   "_default_" : {    "properties" : {     "speaker" : {"type": "string", "index" : "not_analyzed" },     "play_name" : {"type": "string", "index" : "not_analyzed" },     "line_id" : { "type" : "integer" },     "speech_number" : { "type" : "integer" }    }   }  } } ';] failed to create
org.elasticsearch.indices.InvalidIndexNameException: [shakespeare -d ' {  "mappings" : {   "_default_" : {    "properties" : {     "speaker" : {"type": "string", "index" : "not_analyzed" },     "play_name" : {"type": "string", "index" : "not_analyzed" },     "line_id" : { "type" : "integer" },     "speech_number" : { "type" : "integer" }    }   }  } } ';] Invalid index name [shakespeare -d ' {  "mappings" : {   "_default_" : {    "properties" : {     "speaker" : {"type": "string", "index" : "not_analyzed" },     "play_name" : {"type": "string", "index" : "not_analyzed" },     "line_id" : { "type" : "integer" },     "speech_number" : { "type" : "integer" }    }   }  } } ';], must not contain the following characters [\, /, *, ?, ", <, >, |,  , ,]
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validateIndexName(MetaDataCreateIndexService.java:173)
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.validate(MetaDataCreateIndexService.java:559)
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService.access$200(MetaDataCreateIndexService.java:87)
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:243)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
[2014-12-11 20:26:36,521][INFO ][cluster.metadata         ] [Master Menace] [animal] creating index, cause [api], shards [5]/[1], mappings []
[2014-12-11 20:28:52,618][INFO ][cluster.metadata         ] [Master Menace] [animal] create_mapping [badger]
[2014-12-11 21:51:39,472][INFO ][node                     ] [Lament] version[1.4.1], pid[2092], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-11 21:51:39,472][INFO ][node                     ] [Lament] initializing ...
[2014-12-11 21:51:39,475][INFO ][plugins                  ] [Lament] loaded [], sites []
[2014-12-11 21:51:42,402][INFO ][node                     ] [Lament] initialized
[2014-12-11 21:51:42,402][INFO ][node                     ] [Lament] starting ...
[2014-12-11 21:51:42,536][INFO ][transport                ] [Lament] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.12:9300]}
[2014-12-11 21:51:42,664][INFO ][discovery                ] [Lament] elasticsearch/acvZ2TzuTgy-81pJNWxfcQ
[2014-12-11 21:51:46,426][INFO ][cluster.service          ] [Lament] new_master [Lament][acvZ2TzuTgy-81pJNWxfcQ][BlueHorst][inet[/192.168.1.12:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-11 21:51:46,510][INFO ][http                     ] [Lament] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.1.12:9200]}
[2014-12-11 21:51:46,510][INFO ][node                     ] [Lament] started
[2014-12-11 21:51:46,844][INFO ][gateway                  ] [Lament] recovered [2] indices into cluster_state
[2014-12-15 12:57:30,979][INFO ][node                     ] [Toad-In-Waiting] version[1.4.1], pid[6036], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-15 12:57:30,980][INFO ][node                     ] [Toad-In-Waiting] initializing ...
[2014-12-15 12:57:30,988][INFO ][plugins                  ] [Toad-In-Waiting] loaded [], sites []
[2014-12-15 12:57:36,076][INFO ][node                     ] [Toad-In-Waiting] initialized
[2014-12-15 12:57:36,077][INFO ][node                     ] [Toad-In-Waiting] starting ...
[2014-12-15 12:57:36,550][INFO ][transport                ] [Toad-In-Waiting] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/141.22.31.157:9300]}
[2014-12-15 12:57:36,806][INFO ][discovery                ] [Toad-In-Waiting] elasticsearch/8vP2a0BeRyan5dbC7H-n_A
[2014-12-15 12:57:40,612][INFO ][cluster.service          ] [Toad-In-Waiting] new_master [Toad-In-Waiting][8vP2a0BeRyan5dbC7H-n_A][SALLY][inet[/141.22.31.157:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-15 12:57:40,747][INFO ][http                     ] [Toad-In-Waiting] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/141.22.31.157:9200]}
[2014-12-15 12:57:40,748][INFO ][node                     ] [Toad-In-Waiting] started
[2014-12-15 12:57:42,036][INFO ][gateway                  ] [Toad-In-Waiting] recovered [2] indices into cluster_state
[2014-12-15 13:00:07,261][INFO ][node                     ] [Toad-In-Waiting] stopping ...
[2014-12-15 13:00:07,495][INFO ][node                     ] [Toad-In-Waiting] stopped
[2014-12-15 13:00:07,495][INFO ][node                     ] [Toad-In-Waiting] closing ...
[2014-12-15 13:00:07,510][INFO ][node                     ] [Toad-In-Waiting] closed
[2014-12-15 13:14:45,927][INFO ][node                     ] [U-Man] version[1.4.1], pid[6228], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-15 13:14:45,942][INFO ][node                     ] [U-Man] initializing ...
[2014-12-15 13:14:45,942][INFO ][plugins                  ] [U-Man] loaded [], sites []
[2014-12-15 13:14:51,059][INFO ][node                     ] [U-Man] initialized
[2014-12-15 13:14:51,059][INFO ][node                     ] [U-Man] starting ...
[2014-12-15 13:14:51,512][INFO ][transport                ] [U-Man] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/141.22.31.157:9300]}
[2014-12-15 13:14:51,683][INFO ][discovery                ] [U-Man] elasticsearch/LThXAAK3TG2Q0t6wC6tDNw
[2014-12-15 13:14:55,495][INFO ][cluster.service          ] [U-Man] new_master [U-Man][LThXAAK3TG2Q0t6wC6tDNw][SALLY][inet[/141.22.31.157:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-15 13:14:55,627][INFO ][http                     ] [U-Man] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/141.22.31.157:9200]}
[2014-12-15 13:14:55,630][INFO ][node                     ] [U-Man] started
[2014-12-15 13:14:56,900][INFO ][gateway                  ] [U-Man] recovered [2] indices into cluster_state
[2014-12-15 13:21:49,595][INFO ][node                     ] [U-Man] stopping ...
[2014-12-15 13:21:49,892][INFO ][node                     ] [U-Man] stopped
[2014-12-15 13:21:49,892][INFO ][node                     ] [U-Man] closing ...
[2014-12-15 13:21:49,892][INFO ][node                     ] [U-Man] closed
[2014-12-16 13:25:28,079][INFO ][node                     ] [Death-Stalker] version[1.4.1], pid[8180], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-16 13:25:28,080][INFO ][node                     ] [Death-Stalker] initializing ...
[2014-12-16 13:25:28,083][INFO ][plugins                  ] [Death-Stalker] loaded [], sites []
[2014-12-16 13:25:31,315][INFO ][node                     ] [Death-Stalker] initialized
[2014-12-16 13:25:31,316][INFO ][node                     ] [Death-Stalker] starting ...
[2014-12-16 13:25:31,549][INFO ][transport                ] [Death-Stalker] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.178.34:9300]}
[2014-12-16 13:25:31,846][INFO ][discovery                ] [Death-Stalker] elasticsearch/aRmYUvJKTQKTe41hCqFSow
[2014-12-16 13:25:35,611][INFO ][cluster.service          ] [Death-Stalker] new_master [Death-Stalker][aRmYUvJKTQKTe41hCqFSow][Fabian-PC][inet[/192.168.178.34:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-16 13:25:35,641][INFO ][gateway                  ] [Death-Stalker] recovered [1] indices into cluster_state
[2014-12-16 13:25:35,773][INFO ][http                     ] [Death-Stalker] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.178.34:9200]}
[2014-12-16 13:25:35,773][INFO ][node                     ] [Death-Stalker] started
[2014-12-16 13:26:05,615][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:26:05,616][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:26:35,613][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:27:05,614][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:27:35,610][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:27:35,610][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:28:05,610][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:28:35,608][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:29:05,609][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:29:05,609][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:29:35,609][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:30:05,605][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:30:35,607][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:30:35,607][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:31:05,601][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:31:35,601][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:32:05,600][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:32:05,600][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:32:35,596][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:33:05,597][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:33:35,591][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:33:35,591][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:34:05,592][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:34:35,591][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:35:05,591][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:35:05,591][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:35:35,592][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:36:05,586][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:36:35,586][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:36:35,586][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:37:05,587][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:37:35,588][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:37:35,588][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:38:05,589][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:38:35,591][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:38:35,592][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:39:05,592][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:39:35,594][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:39:35,594][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:40:05,596][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:40:35,597][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:40:35,597][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:41:05,599][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:41:35,600][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:41:35,600][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:42:05,601][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:42:35,602][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:42:35,602][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:43:05,603][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:43:35,605][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:43:35,605][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:44:05,604][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:44:35,605][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:45:05,603][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:45:05,603][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:45:11,485][WARN ][transport.netty          ] [Death-Stalker] exception caught on transport layer [[id: 0x19588e36, /0:0:0:0:0:0:0:1:52561 => /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 13:45:11,488][WARN ][transport.netty          ] [Death-Stalker] exception caught on transport layer [[id: 0x19588e36, /0:0:0:0:0:0:0:1:52561 :> /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 13:45:11,490][WARN ][transport.netty          ] [Death-Stalker] exception caught on transport layer [[id: 0x20073622, /0:0:0:0:0:0:0:1:52562 => /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 13:45:11,494][WARN ][transport.netty          ] [Death-Stalker] exception caught on transport layer [[id: 0x20073622, /0:0:0:0:0:0:0:1:52562 :> /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 13:45:11,495][WARN ][transport.netty          ] [Death-Stalker] exception caught on transport layer [[id: 0x9f7fda18, /0:0:0:0:0:0:0:1:52563 => /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 13:45:11,508][WARN ][transport.netty          ] [Death-Stalker] exception caught on transport layer [[id: 0x9f7fda18, /0:0:0:0:0:0:0:1:52563 :> /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 13:45:11,564][WARN ][transport.netty          ] [Death-Stalker] exception caught on transport layer [[id: 0x513985d3, /0:0:0:0:0:0:0:1:52564 => /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 13:45:11,566][WARN ][transport.netty          ] [Death-Stalker] exception caught on transport layer [[id: 0x513985d3, /0:0:0:0:0:0:0:1:52564 :> /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 13:45:35,601][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:46:05,602][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:46:35,601][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:46:35,601][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:47:05,603][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:47:35,601][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:48:05,603][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:48:05,603][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:48:35,605][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:49:05,603][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:49:35,603][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:49:35,603][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:50:05,596][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:50:35,597][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:51:05,595][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:51:05,595][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:51:35,594][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:52:05,593][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:52:35,594][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:52:35,594][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:53:05,594][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:53:35,596][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:53:35,596][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:54:05,594][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:54:35,593][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:55:05,589][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:55:05,589][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:55:35,595][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:56:05,591][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:56:05,591][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:56:35,591][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:57:05,593][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:57:05,593][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:57:35,595][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:58:05,597][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:58:05,597][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:58:35,598][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:59:05,600][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 13:59:05,600][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 13:59:35,601][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:00:05,602][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:00:05,602][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:00:35,604][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:01:05,606][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:01:05,606][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:01:35,608][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:02:05,609][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:02:05,609][INFO ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:02:35,611][WARN ][cluster.routing.allocation.decider] [Death-Stalker] high disk watermark [10%] exceeded on [aRmYUvJKTQKTe41hCqFSow][Death-Stalker] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:47:53,018][INFO ][node                     ] [Eleggua] version[1.4.1], pid[5204], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-16 14:47:53,019][INFO ][node                     ] [Eleggua] initializing ...
[2014-12-16 14:47:53,022][INFO ][plugins                  ] [Eleggua] loaded [], sites []
[2014-12-16 14:47:55,919][INFO ][node                     ] [Eleggua] initialized
[2014-12-16 14:47:55,919][INFO ][node                     ] [Eleggua] starting ...
[2014-12-16 14:47:56,139][INFO ][transport                ] [Eleggua] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.178.34:9300]}
[2014-12-16 14:47:56,433][INFO ][discovery                ] [Eleggua] elasticsearch/TLcCPFb3ROe9T4bjBpwUqw
[2014-12-16 14:48:00,196][INFO ][cluster.service          ] [Eleggua] new_master [Eleggua][TLcCPFb3ROe9T4bjBpwUqw][Fabian-PC][inet[/192.168.178.34:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-16 14:48:00,381][INFO ][http                     ] [Eleggua] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.178.34:9200]}
[2014-12-16 14:48:00,381][INFO ][node                     ] [Eleggua] started
[2014-12-16 14:48:00,571][INFO ][gateway                  ] [Eleggua] recovered [1] indices into cluster_state
[2014-12-16 14:48:30,208][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 3gb[2.7%], shards will be relocated away from this node
[2014-12-16 14:48:30,208][INFO ][cluster.routing.allocation.decider] [Eleggua] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:49:00,203][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 3gb[2.7%], shards will be relocated away from this node
[2014-12-16 14:49:30,204][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 3gb[2.7%], shards will be relocated away from this node
[2014-12-16 14:50:00,205][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 3gb[2.7%], shards will be relocated away from this node
[2014-12-16 14:50:00,205][INFO ][cluster.routing.allocation.decider] [Eleggua] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:50:30,206][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 3gb[2.7%], shards will be relocated away from this node
[2014-12-16 14:51:00,207][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:51:00,207][INFO ][cluster.routing.allocation.decider] [Eleggua] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:51:30,209][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:52:00,211][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:52:00,211][INFO ][cluster.routing.allocation.decider] [Eleggua] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:52:30,211][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:53:00,212][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:53:00,212][INFO ][cluster.routing.allocation.decider] [Eleggua] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:53:30,214][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:54:00,214][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:54:00,214][INFO ][cluster.routing.allocation.decider] [Eleggua] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:54:30,216][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:55:00,218][WARN ][cluster.routing.allocation.decider] [Eleggua] high disk watermark [10%] exceeded on [TLcCPFb3ROe9T4bjBpwUqw][Eleggua] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:55:00,218][INFO ][cluster.routing.allocation.decider] [Eleggua] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:55:59,404][INFO ][node                     ] [Black Crow] version[1.4.1], pid[3760], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-16 14:55:59,404][INFO ][node                     ] [Black Crow] initializing ...
[2014-12-16 14:55:59,408][INFO ][plugins                  ] [Black Crow] loaded [], sites []
[2014-12-16 14:56:02,325][INFO ][node                     ] [Black Crow] initialized
[2014-12-16 14:56:02,325][INFO ][node                     ] [Black Crow] starting ...
[2014-12-16 14:56:02,557][INFO ][transport                ] [Black Crow] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.178.34:9300]}
[2014-12-16 14:56:02,863][INFO ][discovery                ] [Black Crow] elasticsearch/SGBRIXBZQwOZQuW3YmYEIQ
[2014-12-16 14:56:06,627][INFO ][cluster.service          ] [Black Crow] new_master [Black Crow][SGBRIXBZQwOZQuW3YmYEIQ][Fabian-PC][inet[/192.168.178.34:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-16 14:56:06,795][INFO ][http                     ] [Black Crow] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.178.34:9200]}
[2014-12-16 14:56:06,795][INFO ][node                     ] [Black Crow] started
[2014-12-16 14:56:06,985][INFO ][gateway                  ] [Black Crow] recovered [1] indices into cluster_state
[2014-12-16 14:56:36,639][WARN ][cluster.routing.allocation.decider] [Black Crow] high disk watermark [10%] exceeded on [SGBRIXBZQwOZQuW3YmYEIQ][Black Crow] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:56:36,639][INFO ][cluster.routing.allocation.decider] [Black Crow] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:57:06,633][WARN ][cluster.routing.allocation.decider] [Black Crow] high disk watermark [10%] exceeded on [SGBRIXBZQwOZQuW3YmYEIQ][Black Crow] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:57:36,635][WARN ][cluster.routing.allocation.decider] [Black Crow] high disk watermark [10%] exceeded on [SGBRIXBZQwOZQuW3YmYEIQ][Black Crow] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:58:06,636][WARN ][cluster.routing.allocation.decider] [Black Crow] high disk watermark [10%] exceeded on [SGBRIXBZQwOZQuW3YmYEIQ][Black Crow] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:58:06,636][INFO ][cluster.routing.allocation.decider] [Black Crow] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 14:58:36,638][WARN ][cluster.routing.allocation.decider] [Black Crow] high disk watermark [10%] exceeded on [SGBRIXBZQwOZQuW3YmYEIQ][Black Crow] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:59:06,640][WARN ][cluster.routing.allocation.decider] [Black Crow] high disk watermark [10%] exceeded on [SGBRIXBZQwOZQuW3YmYEIQ][Black Crow] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 14:59:06,640][INFO ][cluster.routing.allocation.decider] [Black Crow] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 15:03:12,515][INFO ][node                     ] [Metal Master] version[1.4.1], pid[3416], build[89d3241/2014-11-26T15:49:29Z]
[2014-12-16 15:03:12,515][INFO ][node                     ] [Metal Master] initializing ...
[2014-12-16 15:03:12,519][INFO ][plugins                  ] [Metal Master] loaded [], sites []
[2014-12-16 15:03:15,400][INFO ][node                     ] [Metal Master] initialized
[2014-12-16 15:03:15,400][INFO ][node                     ] [Metal Master] starting ...
[2014-12-16 15:03:15,619][INFO ][transport                ] [Metal Master] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.178.34:9300]}
[2014-12-16 15:03:15,913][INFO ][discovery                ] [Metal Master] elasticsearch/4CSp51oYRn-ad6IXBAlqcQ
[2014-12-16 15:03:19,674][INFO ][cluster.service          ] [Metal Master] new_master [Metal Master][4CSp51oYRn-ad6IXBAlqcQ][Fabian-PC][inet[/192.168.178.34:9300]], reason: zen-disco-join (elected_as_master)
[2014-12-16 15:03:19,841][INFO ][http                     ] [Metal Master] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.178.34:9200]}
[2014-12-16 15:03:19,841][INFO ][node                     ] [Metal Master] started
[2014-12-16 15:03:20,016][INFO ][gateway                  ] [Metal Master] recovered [1] indices into cluster_state
[2014-12-16 15:03:23,153][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x0d8c7996, /0:0:0:0:0:0:0:1:55011 => /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:23,156][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x0d8c7996, /0:0:0:0:0:0:0:1:55011 :> /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:38,417][DEBUG][action.admin.indices.create] [Metal Master] [mps] failed to create
org.elasticsearch.index.mapper.MapperParsingException: mapping [data]
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:405)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: Root type mapping not empty after parsing! Remaining fields:   [istEinAuftrag : false] [timestamps : {fertigStellung=0, ausLieferung=0, zahlungsEingang=0, auftragsErteilung=0}] [storniert : false] [neuKunde : false]
	at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:276)
	at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:190)
	at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:444)
	at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:317)
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:402)
	... 5 more
[2014-12-16 15:03:40,239][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xb8aae021, /127.0.0.1:55041 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,240][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x1077b7e5, /127.0.0.1:55038 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,240][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x1978c632, /127.0.0.1:55045 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,240][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x78882982, /127.0.0.1:55039 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,239][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x0dd3a213, /127.0.0.1:55042 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,250][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x282cdaf7, /127.0.0.1:55050 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,239][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x9def729d, /127.0.0.1:55040 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,239][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xfa160bef, /127.0.0.1:55044 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,239][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xdad91192, /127.0.0.1:55043 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,253][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x250d1b42, /127.0.0.1:55048 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,248][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xb57bba19, /127.0.0.1:55047 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,244][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xff42ba3a, /127.0.0.1:55046 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,242][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x92ed3f83, /127.0.0.1:55049 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:40,257][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x4608031a, /127.0.0.1:55051 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:03:49,685][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:03:49,685][INFO ][cluster.routing.allocation.decider] [Metal Master] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 15:04:02,467][DEBUG][action.admin.indices.create] [Metal Master] [mps] failed to create
org.elasticsearch.index.mapper.MapperParsingException: mapping [data]
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:405)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: Root type mapping not empty after parsing! Remaining fields:   [istEinAuftrag : false] [timestamps : {fertigStellung=0, ausLieferung=0, zahlungsEingang=0, auftragsErteilung=0}] [storniert : false] [neuKunde : false]
	at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:276)
	at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:190)
	at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:444)
	at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:317)
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:402)
	... 5 more
[2014-12-16 15:04:07,380][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xbafa5a44, /127.0.0.1:55090 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,380][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x6a9067ba, /127.0.0.1:55092 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,380][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xf3babc82, /127.0.0.1:55086 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,380][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x17c5d29a, /127.0.0.1:55088 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,380][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x3dffd237, /127.0.0.1:55089 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,380][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x26c862b6, /127.0.0.1:55087 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,380][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x67d0769d, /127.0.0.1:55091 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,380][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x8f8cdc3a, /127.0.0.1:55085 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,387][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xf3a20fa8, /127.0.0.1:55095 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,386][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xbd2a52c8, /127.0.0.1:55097 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,385][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xf7334b08, /127.0.0.1:55096 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,384][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x6427bdb9, /127.0.0.1:55094 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,381][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xc222ee21, /127.0.0.1:55098 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:07,397][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xc2611fdb, /127.0.0.1:55093 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:19,682][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:04:23,162][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xd33a3614, /0:0:0:0:0:0:0:1:55104 => /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:23,164][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xd33a3614, /0:0:0:0:0:0:0:1:55104 :> /0:0:0:0:0:0:0:1:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format, got (47,45,54,20)
	at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:47)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:360)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
	at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
	at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
	at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
	at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:622)
	at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:234)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
	at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
	at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
	at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:04:49,683][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:05:19,684][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:05:19,684][INFO ][cluster.routing.allocation.decider] [Metal Master] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 15:05:49,685][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:06:19,687][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:06:19,687][INFO ][cluster.routing.allocation.decider] [Metal Master] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 15:06:49,688][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:07:19,690][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:07:19,690][INFO ][cluster.routing.allocation.decider] [Metal Master] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 15:07:46,401][DEBUG][action.admin.indices.create] [Metal Master] [mps] failed to create
org.elasticsearch.index.mapper.MapperParsingException: mapping [data]
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:405)
	at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
	at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: Root type mapping not empty after parsing! Remaining fields:   [istEinAuftrag : false] [timestamps : {fertigStellung=0, ausLieferung=0, zahlungsEingang=0, auftragsErteilung=0}] [storniert : false] [neuKunde : false]
	at org.elasticsearch.index.mapper.DocumentMapperParser.parse(DocumentMapperParser.java:276)
	at org.elasticsearch.index.mapper.DocumentMapperParser.parseCompressed(DocumentMapperParser.java:190)
	at org.elasticsearch.index.mapper.MapperService.parse(MapperService.java:444)
	at org.elasticsearch.index.mapper.MapperService.merge(MapperService.java:317)
	at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:402)
	... 5 more
[2014-12-16 15:07:48,399][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xd1357011, /127.0.0.1:55165 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,399][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x087ba052, /127.0.0.1:55172 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,399][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x583c5c3a, /127.0.0.1:55169 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,399][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xce520f01, /127.0.0.1:55168 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,399][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x3250e2bf, /127.0.0.1:55167 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,399][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x1d4ab7cb, /127.0.0.1:55171 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,399][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x10560a84, /127.0.0.1:55170 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,399][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xcd6ba7ae, /127.0.0.1:55166 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,406][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x34159dbd, /127.0.0.1:55178 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,404][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x0d16beac, /127.0.0.1:55175 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,403][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0xc3ec6ed7, /127.0.0.1:55176 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,402][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x9d38665c, /127.0.0.1:55177 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,400][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x437a66cf, /127.0.0.1:55173 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:48,424][WARN ][transport.netty          ] [Metal Master] exception caught on transport layer [[id: 0x772e5714, /127.0.0.1:55174 => /127.0.0.1:9300]], closing connection
java.io.IOException: Eine vorhandene Verbindung wurde vom Remotehost geschlossen
	at sun.nio.ch.SocketDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225)
	at sun.nio.ch.IOUtil.read(IOUtil.java:193)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
[2014-12-16 15:07:49,691][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:08:19,693][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
[2014-12-16 15:08:19,693][INFO ][cluster.routing.allocation.decider] [Metal Master] high disk watermark exceeded on one or more nodes, rerouting shards
[2014-12-16 15:08:49,694][WARN ][cluster.routing.allocation.decider] [Metal Master] high disk watermark [10%] exceeded on [4CSp51oYRn-ad6IXBAlqcQ][Metal Master] free: 2.9gb[2.6%], shards will be relocated away from this node
